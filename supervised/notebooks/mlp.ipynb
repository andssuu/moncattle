{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNRnJT1gVyEmiaUyZUqMzbg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andssuu/moncattle/blob/master/supervised/notebooks/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEXFWKq-uvUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2168f3e0-0e2c-459e-b3a6-de2750582cc7"
      },
      "source": [
        "!git clone https://github.com/andssuu/moncattle.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'moncattle'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 128 (delta 42), reused 82 (delta 28), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (128/128), 980.58 KiB | 15.32 MiB/s, done.\n",
            "Resolving deltas: 100% (42/42), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN6UdKHfuNJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03f9441-f520-4234-d7cf-c0a718536ef5"
      },
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "def sigmoid(net):\n",
        "    return 1.0/(1.0+np.exp(-net))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n",
        "\n",
        "class Network(object):\n",
        "\n",
        "    def __init__(self, sizes, activation_function=sigmoid,\n",
        "                 prime_function=sigmoid_prime):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(\n",
        "            sizes[:-1], sizes[1:])]\n",
        "        self.activation_function = activation_function\n",
        "        self.prime_function = prime_function\n",
        "\n",
        "    def feedforward(self, x):\n",
        "        \"\"\"Retorna a sa√≠da da rede z se `x` for entrada.\"\"\"\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            x = self.activation_function(np.dot(w, x)+b)  # net = (‚àëxw+b)\n",
        "        return x\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, _n, test_data=None):\n",
        "        n = len(training_data)\n",
        "        n_test = len(test_data)\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [training_data[k:k+mini_batch_size]\n",
        "                            for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, _n)\n",
        "            if test_data:\n",
        "                acc = self.evaluate(test_data)\n",
        "                print(\"Epoch {} : {} / {} = {}%\".format(j,\n",
        "                                                        acc, n_test, (acc*100)/n_test))\n",
        "            else:\n",
        "                print(\"Epoch {} finalizada\".format(j))\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, _n):\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "        self.weights = [w-(_n/len(mini_batch))*nw for w,\n",
        "                        nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(_n/len(mini_batch))*nb for b,\n",
        "                       nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Retorna uma tupla `(nabla_b, nabla_w)` representando o\n",
        "         gradiente para a fun√ß√£o de custo J_x. `nabla_b` e\n",
        "         `nabla_w` s√£o listas de camadas de matrizes numpy, semelhantes\n",
        "         a `self.biases` e `self.weights`.\"\"\"\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        nets = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            net = np.dot(w, activation)+b\n",
        "            nets.append(net)\n",
        "            activation = self.activation_function(net)\n",
        "            activations.append(activation)\n",
        "        # Backward pass\n",
        "        delta = self.cost_derivative(\n",
        "            activations[-1], y) * self.prime_function(nets[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        # (ùë¶‚àíùëß)*f‚Äô(net)*ùë•\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        for l in range(2, self.num_layers):\n",
        "            net = nets[-l]\n",
        "            zs = self.prime_function(net)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * zs\n",
        "            nabla_b[-l] = delta\n",
        "            # ‚àë(ùõøùë§)f‚Äô(net)ùë•\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in test_data]\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "    def cost_derivative(self, output_activations, y): return (output_activations-y)\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    url = 'moncattle/data/lomba.csv'\n",
        "    df = pd.read_csv(url)\n",
        "    data = df[df.columns[1:10]]\n",
        "    # normaliza os dados\n",
        "    normalized_data = (data - data.min()) / (data.max() - data.min())\n",
        "    labels = df[df.columns[-1]]\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        normalized_data, labels, test_size=0.4, random_state=0)\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    le.fit(y_train.values)\n",
        "    y_train = [vectorized_result(y) for y in le.transform(y_train.values)]\n",
        "    x_train = [np.reshape(x, (9, 1)) for x in x_train.values]\n",
        "    x_test = [np.reshape(x, (9, 1)) for x in x_test.values]\n",
        "    return zip(x_train, y_train), zip(x_test, le.transform(y_test.values))\n",
        "\n",
        "\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((4, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    training_data, test_data = load_data()\n",
        "    arquitecture = [9, 30, 20, 4]\n",
        "    mlp = Network(arquitecture)\n",
        "    mlp.SGD(list(training_data), 20, 24, 0.3, test_data=list(test_data))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 : 2316 / 5236 = 44.23223834988541%\n",
            "Epoch 1 : 2927 / 5236 = 55.901451489686785%\n",
            "Epoch 2 : 2994 / 5236 = 57.18105423987777%\n",
            "Epoch 3 : 3331 / 5236 = 63.617265087853326%\n",
            "Epoch 4 : 3670 / 5236 = 70.0916730328495%\n",
            "Epoch 5 : 3541 / 5236 = 67.62796027501909%\n",
            "Epoch 6 : 3792 / 5236 = 72.42169595110772%\n",
            "Epoch 7 : 3801 / 5236 = 72.59358288770053%\n",
            "Epoch 8 : 3773 / 5236 = 72.05882352941177%\n",
            "Epoch 9 : 3576 / 5236 = 68.29640947288006%\n",
            "Epoch 10 : 3684 / 5236 = 70.35905271199388%\n",
            "Epoch 11 : 3741 / 5236 = 71.44766997708174%\n",
            "Epoch 12 : 3802 / 5236 = 72.61268143621085%\n",
            "Epoch 13 : 3741 / 5236 = 71.44766997708174%\n",
            "Epoch 14 : 3704 / 5236 = 70.74102368220015%\n",
            "Epoch 15 : 3800 / 5236 = 72.57448433919022%\n",
            "Epoch 16 : 3732 / 5236 = 71.27578304048892%\n",
            "Epoch 17 : 3750 / 5236 = 71.61955691367456%\n",
            "Epoch 18 : 3678 / 5236 = 70.24446142093201%\n",
            "Epoch 19 : 3742 / 5236 = 71.46676852559206%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}